{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The Dataset API makes it easy to deal with large amounts of data, different data formats, and complicated transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.data.Dataset: A sequence of elements.\n",
    "\n",
    "* elements: each elements contains one or more Tenor objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np_array_1 = np.array([1,2,3,4])\n",
    "np_array_2 = np.array([5,6,7,8])\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np_array_1, np_array_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorShape([]), TensorShape([])), (tf.int64, tf.int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_shapes, dataset.output_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a simpler api for just 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np_array_1 = np.array([1,2,3,4])\n",
    "\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((np_array_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([]), tf.int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.output_shapes, dataset.output_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of your input data fit in memory, the simplest way to create a Dataset from them is to convert them to tf.Tensor objects and use Dataset.from_tensor_slices()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From TFRecords data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tf.data.TFRecordDataset class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset that reads all of the examples from two files.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filenames argument to the TFRecordDataset initializer can either be a string, a list of strings, or a **tf.Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can use a tf.placeholder(tf.string) to represent the filenames, and initialize an iterator from the appropriate filenames**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Consuming text data: tf.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many datasets are distributed as one or more text files. The tf.data.TextLineDataset provides an easy way to extract lines from one or more text files. Given one or more filenames, a TextLineDataset will produce one string-valued element per line of those files. Like a TFRecordDataset, TextLineDataset accepts filenames as a tf.Tensor, so you can parameterize it by passing a tf.placeholder(tf.string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data with Dataset.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing tf.Example protocol buffer messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many input pipelines extract tf.train.Example protocol buffer messages from a TFRecord-format file (written, for example, using tf.python_io.TFRecordWriter). Each tf.train.Example record contains one or more \"features\", and the input pipeline typically converts these features into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an image and its label, respectively.\n",
    "def _parse_function(example_proto):\n",
    "  features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "              \"label\": tf.FixedLenFeature((), tf.int32, default_value=0)}\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  return parsed_features[\"image\"], parsed_features[\"label\"]\n",
    "\n",
    "# Creates a dataset that reads all of the examples from two files, and extracts\n",
    "# the image and label features.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying arbitrary Python logic with tf.py_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching dataset elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[4 5 6 7]\n",
      "[ 8  9 10 11]\n",
      "[12 13 14 15]\n",
      "[16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "dataset = tf.data.Dataset.range(20)\n",
    "data_batch = dataset.batch(4)\n",
    "\n",
    "iterator = data_batch.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with sess.as_default() as sess:\n",
    "#     tf.train.start_queue_runners()\n",
    "\n",
    "    for i in range(5):\n",
    "      value = sess.run(next_element)\n",
    "      print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use tf.train.shuffle_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 15 14  1]\n",
      "[16  9 19 13]\n",
      "[6 3 4 0]\n",
      "[12  2 10  8]\n",
      "[ 7 17 18 11]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "dataset = tf.data.Dataset.range(20)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "# next_element.set_shape([1])\n",
    "\n",
    "batch_elment = tf.train.shuffle_batch([next_element], batch_size=4, capacity=64, min_after_dequeue=32)\n",
    "with sess.as_default() as sess:\n",
    "    tf.train.start_queue_runners()\n",
    "    for i in range(5):\n",
    "      value = sess.run(batch_elment)\n",
    "      print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly shuffling input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 56 72 70  6 65 31 55 14 18 62  2 74 46 67 11]\n",
      "[76 17 13 77  7 43  5 79 26 42 25 37 48 52 28 36]\n",
      "[15 33 45 57 16 24 22 15 27 62 12 71 68  3 60 11]\n",
      "[75 31 51  0 76 14 44 66 78 12 50 19 30 64 68 70]\n",
      "[47 36  9 55 18 23 47 72 45 21 44 52 79  8 29 59]\n",
      "[41 35 63 20 25 28 10 19 59 26 46 60 16 48 10 61]\n",
      "[39 65  8 39 73 53 64 78 57  5 66 54 69 49 40 30]\n",
      "[ 6 69 20 74 33 22  2  4 13  3 77 35  1  0 38 67]\n",
      "[50 71 23 58 38 41 24 17 29  7 75 42 56 21 54 51]\n",
      "[61  1 37 32 43 73 27 49 34 32 53  4  9 63 58 40]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "dataset = tf.data.Dataset.range(80)\n",
    "\n",
    "dataset = dataset.repeat(2)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=160)\n",
    "\n",
    "dataset = dataset.batch(16)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# next_element.set_shape([1])\n",
    "\n",
    "with sess.as_default() as sess:\n",
    "    for i in range(10):\n",
    "      value = sess.run(next_element)\n",
    "      print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
